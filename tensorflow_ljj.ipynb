{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "def early_stopping(accuracy,EARLY_STOPPING):\n",
    "        stop_cnt=0\n",
    "        stop_max = np.argmax(accuracy)\n",
    "        stop_len = len(accuracy)\n",
    "        for i in range(stop_len-1,max(stop_max,stop_len-1-EARLY_STOPPING),-1):\n",
    "            if accuracy[i]<accuracy[stop_max]:\n",
    "                stop_cnt=stop_cnt+1\n",
    "        if stop_cnt>=EARLY_STOPPING:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=64, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    #counter\n",
    "    iter_cnt=0\n",
    "    \n",
    "    #add by ljj\n",
    "    EARLY_STOPPING = 5\n",
    "    eStopAcc = []\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%X_train.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[i:i+batch_size].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                pass\n",
    "#                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\".format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "#        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\".format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "        #for early stopping add bi ljj \n",
    "        eStopAcc.append(total_correct)\n",
    "        if early_stopping(eStopAcc,EARLY_STOPPING)==1:\n",
    "            break\n",
    "    return total_loss,total_correct\n",
    "\n",
    "# Feel free to play with this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a _great_ model on CIFAR-10!\n",
    "\n",
    "Now it's your job to experiment with architectures, hyperparameters, loss functions, and optimizers to train a model that achieves ** >= 70% accuracy on the validation set** of CIFAR-10. You can use the `run_model` function from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things you should try:\n",
    "- **Filter size**: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- **Number of filters**: Above we used 32 filters. Do more or fewer do better?\n",
    "- **Pooling vs Strided Convolution**: Do you use max pooling or just stride convolutions?\n",
    "- **Batch normalization**: Try adding spatial batch normalization after convolution layers and vanilla batch normalization after affine layers. Do your networks train faster?\n",
    "- **Network architecture**: The network above has two layers of trainable parameters. Can you do better with a deep network? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Use TensorFlow Scope**: Use TensorFlow scope and/or [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) to make it easier to write deeper networks. See [this tutorial](https://www.tensorflow.org/tutorials/layers) for making how to use `tf.layers`. \n",
    "- **Use Learning Rate Decay**: [As the notes point out](http://cs231n.github.io/neural-networks-3/#anneal), decaying the learning rate might help the model converge. Feel free to decay every epoch, when loss doesn't change over an entire epoch, or any other heuristic you find appropriate. See the [Tensorflow documentation](https://www.tensorflow.org/versions/master/api_guides/python/train#Decaying_the_learning_rate) for learning rate decay.\n",
    "- **Global Average Pooling**: Instead of flattening and then having multiple affine layers, perform convolutions until your image gets small (7x7 or so) and then perform an average pooling operation to get to a 1x1 image picture (1, 1 , Filter#), which is then reshaped into a (Filter#) vector. This is used in [Google's Inception Network](https://arxiv.org/abs/1512.00567) (See Table 1 for their architecture).\n",
    "- **Regularization**: Add l2 weight regularization, or perhaps use [Dropout as in the TensorFlow MNIST tutorial](https://www.tensorflow.org/get_started/mnist/pros)\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and we'll save the test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at **>= 70% accuracy on the validation set**. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training and validation set accuracies for your final trained network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'lr':[5e-3,1e-3,5e-4],\n",
    "         'decay':[0.99,0.9,0.8],\n",
    "         'momentum':[0,0.1,0.2],\n",
    "         'is_reg':[1,0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.005 decay = 0.99 momentum = 0 is_reg = 1 stop@Epoch 18,  train_accuracy = 0.969 val_acc = 0.835 test_acc = 0.822\n",
      "lr = 0.005 decay = 0.99 momentum = 0 is_reg = 0 stop@Epoch 28,  train_accuracy = 0.981 val_acc = 0.826 test_acc = 0.825\n",
      "lr = 0.005 decay = 0.99 momentum = 0.1 is_reg = 1 stop@Epoch 17,  train_accuracy = 0.965 val_acc = 0.814 test_acc = 0.81\n",
      "lr = 0.005 decay = 0.99 momentum = 0.1 is_reg = 0 stop@Epoch 17,  train_accuracy = 0.965 val_acc = 0.829 test_acc = 0.821\n",
      "lr = 0.005 decay = 0.99 momentum = 0.2 is_reg = 1 stop@Epoch 12,  train_accuracy = 0.945 val_acc = 0.813 test_acc = 0.809\n",
      "lr = 0.005 decay = 0.99 momentum = 0.2 is_reg = 0 stop@Epoch 15,  train_accuracy = 0.96 val_acc = 0.818 test_acc = 0.812\n",
      "lr = 0.005 decay = 0.9 momentum = 0 is_reg = 1 stop@Epoch 11,  train_accuracy = 0.944 val_acc = 0.805 test_acc = 0.791\n",
      "lr = 0.005 decay = 0.9 momentum = 0 is_reg = 0 stop@Epoch 15,  train_accuracy = 0.962 val_acc = 0.817 test_acc = 0.79\n",
      "lr = 0.005 decay = 0.9 momentum = 0.1 is_reg = 1 stop@Epoch 13,  train_accuracy = 0.954 val_acc = 0.807 test_acc = 0.805\n",
      "lr = 0.005 decay = 0.9 momentum = 0.1 is_reg = 0 stop@Epoch 13,  train_accuracy = 0.951 val_acc = 0.819 test_acc = 0.808\n",
      "lr = 0.005 decay = 0.9 momentum = 0.2 is_reg = 1 stop@Epoch 18,  train_accuracy = 0.967 val_acc = 0.796 test_acc = 0.819\n",
      "lr = 0.005 decay = 0.9 momentum = 0.2 is_reg = 0 stop@Epoch 20,  train_accuracy = 0.97 val_acc = 0.82 test_acc = 0.817\n",
      "lr = 0.005 decay = 0.8 momentum = 0 is_reg = 1 stop@Epoch 13,  train_accuracy = 0.949 val_acc = 0.82 test_acc = 0.811\n",
      "lr = 0.005 decay = 0.8 momentum = 0 is_reg = 0 stop@Epoch 15,  train_accuracy = 0.957 val_acc = 0.805 test_acc = 0.807\n",
      "lr = 0.005 decay = 0.8 momentum = 0.1 is_reg = 1 stop@Epoch 12,  train_accuracy = 0.944 val_acc = 0.811 test_acc = 0.795\n",
      "lr = 0.005 decay = 0.8 momentum = 0.1 is_reg = 0 stop@Epoch 16,  train_accuracy = 0.962 val_acc = 0.825 test_acc = 0.815\n"
     ]
    }
   ],
   "source": [
    "for p_lr in params['lr']:\n",
    "    for p_decay in params['decay']:\n",
    "        for p_momentum in params['momentum']:\n",
    "            for p_is_reg in params['is_reg']:\n",
    "                def my_model(X,y,is_training):\n",
    "                    ba0   = tf.layers.batch_normalization(X,training=is_training)\n",
    "                    # [conv-relu-conv-relu-pool]  out=14x14\n",
    "                    conv1 = tf.layers.conv2d(ba0,128,kernel_size=[3,3],strides=(1,1),activation=tf.nn.relu)\n",
    "                    ba1   = tf.layers.batch_normalization(conv1,training=is_training)\n",
    "                    conv2 = tf.layers.conv2d(ba1,256,[3,3],activation=tf.nn.relu)\n",
    "                    ba2   = tf.layers.batch_normalization(conv2,training=is_training)\n",
    "                    pool1 = tf.layers.max_pooling2d(ba2,pool_size=[2,2],strides=2)\n",
    "                    #[conv-relu-conv-relu-pool]  out=5x5\n",
    "                    conv3 = tf.layers.conv2d(pool1,512,[3,3],activation=tf.nn.relu)\n",
    "                    ba3   = tf.layers.batch_normalization(conv3,training=is_training)\n",
    "                    conv4 = tf.layers.conv2d(ba3,256,[3,3],activation=tf.nn.relu)\n",
    "                    ba4   = tf.layers.batch_normalization(conv4,training=is_training)\n",
    "                    pool2 = tf.layers.max_pooling2d(ba4,pool_size=[2,2],strides=2)\n",
    "                    #[dense-relu]x2 layer\n",
    "                    pool2_flat = tf.reshape(pool2,[-1,5*5*256])\n",
    "                    dense1 =tf.layers.dense(pool2_flat,units=512,activation=tf.nn.relu)\n",
    "                    ba5 = tf.layers.batch_normalization(dense1,center=False,scale=False,training=is_training)\n",
    "                    dropout1 = tf.layers.dropout(ba5,training=is_training)\n",
    "                    dense2 = tf.layers.dense(dropout1,units=128,activation=tf.nn.relu)\n",
    "                    ba6 = tf.layers.batch_normalization(dense2,center=False,scale=False,training=is_training)\n",
    "                    dropout2 = tf.layers.dropout(ba6,training=is_training)\n",
    "                    #logit out\n",
    "                    logits = tf.layers.dense(dropout2,units=10)\n",
    "                    return logits\n",
    "                    pass\n",
    "\n",
    "                for i in np.arange(1):\n",
    "                    tf.reset_default_graph()\n",
    "\n",
    "                    X = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "                    y = tf.placeholderl(tf.int64, [None])\n",
    "                    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "                    y_out = my_model(X,y,is_training)\n",
    "                    if p_is_reg:\n",
    "                        total_loss= tf.losses.softmax_cross_entropy(tf.one_hot(y,10),y_out)+tf.losses.get_regularization_loss()\n",
    "                    else:\n",
    "                        total_loss= tf.losses.softmax_cross_entropy(tf.one_hot(y,10),y_out)#+tf.losses.get_regularization_loss()\n",
    "                    mean_loss = tf.reduce_mean(total_loss)\n",
    "                    optimizer = tf.train.RMSPropOptimizer(p_lr,decay=p_decay,momentum=p_momentum)\n",
    "\n",
    "                    # batch normalization in tensorflow requires this extra dependency\n",
    "                    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "                    with tf.control_dependencies(extra_update_ops):\n",
    "                        train_step = optimizer.minimize(mean_loss)\n",
    "\n",
    "                    pass\n",
    "\n",
    "                    # Feel free to play with this cell\n",
    "                    # This default code creates a session\n",
    "                    # and trains your model for 10 epochs\n",
    "                    # then prints the validation set accuracy\n",
    "                    sess = tf.Session()\n",
    "                    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "                    EARLY_STOPPING = 5\n",
    "                    MAX_EPOCH = 50\n",
    "                    eStopAcc = []\n",
    "                    epoch=0\n",
    "                    for e in np.arange(MAX_EPOCH):\n",
    "                        train_loss,train_acc=run_model(sess,y_out,mean_loss,X_train,y_train,1,64,100,train_step)\n",
    "                        val_loss,val_acc=run_model(sess,y_out,mean_loss,X_val,y_val,1,64)\n",
    "                        eStopAcc.append(val_acc)\n",
    "                        epoch=e\n",
    "                        if early_stopping(eStopAcc,EARLY_STOPPING)==1 or e==MAX_EPOCH-1:\n",
    "                            test_loss,test_acc = run_model(sess,y_out,mean_loss,X_test,y_test,1,64)\n",
    "                            break\n",
    "                    print(\"lr = {0:.4g} decay = {1:.2g} momentum = {2:.1g} is_reg = {3} stop@Epoch {4},  train_accuracy = {5:.3g} val_acc = {6:.3g} test_acc = {7:.3g}\".format(p_lr,p_decay,p_momentum,p_is_reg,epoch+1,train_acc,val_acc,test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
